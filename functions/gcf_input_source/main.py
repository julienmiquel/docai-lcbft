import json
import os
import time
from urllib.parse import urlparse

import bq
import docai
import gcs
from function_variables import FunctionVariables
from pdf import split_one_pdf_pages
from pdf_to_jpg import convert_all_pdf_to_jpg

var = FunctionVariables()

def main_run(event, context):
    """[summary]
        Standard cloud function entry 
    Args:
        event ([type]): event with parameters of the cloud function
        context ([type]): context of the cloud function

    Returns:
        [type]: return code of the function
    """    

    #full url address
    gcs_input_uri = 'gs://' + event['bucket'] + '/' + event['name']
    
    if "contentType" not in event:
        print(event)
        event['contentType'] = "None"
        
    print('Printing the contentType: ' + event['contentType'] + ' input:' + gcs_input_uri)
    
    uri = urlparse(gcs_input_uri)
    image_content, blob = gcs.get_raw_bytes(uri)

    # if input is an pdf, we will split each pages in jpg stored in GCS under a folder named with the file name
    # a new event will be generated by gcs.
    if( event['contentType'] == 'application/pdf' ):
        # create tmp folder
        gcs.create_tmp_folders()
        print("convert pdf to image before processing")
        #gcs_uri_in = os.path.join("gs://", event["bucket"], event["name"])
        
        print(f"Preprocessing split from uri: {gcs_input_uri}")
        tmp_pdf_filepaths, success = split_one_pdf_pages(gcs_input_uri)

        tmp_jpg_filepaths = convert_all_pdf_to_jpg(tmp_pdf_filepaths)
        if tmp_jpg_filepaths != []:
            for file in tmp_jpg_filepaths:
                output=os.path.join(gcs_input_uri, os.path.basename(file))
                print(f"Send file: {file} to {output}")
            
                blob = gcs.send_file(output,        file)
                print(f"Successfully sent jpg pages of file{blob.public_url}  ")

        else:
            print("error convert_all_pdf_to_jpg return no files") 
        
        print(f"pdf converted to jpg to {blob.public_url} will now exit")
        # Copy input file to archive bucket
        gcs.backupAndDeleteInput(event)

        return
    
    # start timer
    t0 = time.time()
    
    # if the type of the input file is an json 
    # we will parse the results and store it into BQ
    if( str(event['name']).endswith(".json") or event['contentType'] == 'application/json' ):
        doc_type = "invoice"
        name = "hitl"

        document = docai.docai_extract_doc_from_json(image_content)
        
        entities_extracted_dict = docai.parseDocAIResult(event, gcs_input_uri, uri, t0, doc_type, name, document, None)
        # Write the entities to BQ
        bq.write_to_bq(entities_extracted_dict)


    # if the content is an image, the content will be send to docAI in synchronous way
    # Store the result in BQ
    elif(event['contentType'] == 'image/gif' #or event['contentType'] == 'application/pdf' 
    or event['contentType'] == 'image/tiff' or event['contentType'] == 'image/jpeg'):
        
        doc_type, name = docai.getDocType(gcs_input_uri)
 
        entities_extracted_dict = docai.process_raw_bytes(event, gcs_input_uri, uri, image_content, t0, doc_type, name)
        # Write the entities to BQ
        bq.write_to_bq(entities_extracted_dict)

        delete_blob = False
        if delete_blob == True:
            blob.delete()

        # Check business rules
        if var.CheckBusinessRules:
            bq.checkBusinessRule()
        
        # Copy input file to archive bucket
        if var.BackupAndDeleteInput:
            gcs.backupAndDeleteInput(event)

    else:
        print('Cannot parse the file type')







def main_run_batch(event, context):
    gcs_input_uri = 'gs://' + event['bucket'] + '/' + event['name']
    print('Printing the contentType: ' + event['contentType'] + ' input:' + gcs_input_uri)

    t0 = time.time()

    if(event['contentType'] == 'image/gif' or event['contentType'] == 'application/pdf' 
    or event['contentType'] == 'image/tiff' or event['contentType'] == 'image/jpeg'):
        
        doc_type, blob_list = docai.batch(event, gcs_input_uri)

        for i, blob in enumerate(blob_list):
            # Download the contents of this blob as a bytes object.
            if ".json" not in blob.name:
                print("blob name " + blob.name)
                print(f"skipping non-supported file type {blob.name}")
            else:
                
                # Setting the output file name based on the input file name
                print("Fetching from " + blob.name + " for input_filename " + gcs_input_uri)
                #start = blob.name.rfind("/") + 1
                #end = blob.name.rfind(".") + 1
                input_filename = gcs_input_uri #blob.name[start:end:] + "gif"
                
                
                # Getting ready to read the output of the parsed document - setting up "document"
                blob_as_bytes = blob.download_as_bytes()
                document = json.loads( blob_as_bytes)

                name = "json"

                document = docai.docai_extract_doc_from_json(blob_as_bytes)
                
                entities_extracted_dict = docai.parseDocAIResult(event, gcs_input_uri, urlparse(gcs_input_uri), t0, doc_type, name, document, None)
                # Write the entities to BQ
                bq.write_to_bq(entities_extracted_dict)
                delete_blob = False
                if delete_blob == True:
                    blob.delete()
            
        
        # Copy input file to archive bucket
        gcs.backupAndDeleteInput(event)
    else:
        print('Cannot parse the file type')











